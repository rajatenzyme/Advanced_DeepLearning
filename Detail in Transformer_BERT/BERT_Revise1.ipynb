{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import BertTokenizer\n",
    "from transformers.models.bert.modeling_bert import BertEncoder, BertPooler, BertEmbeddings, BaseModelOutputWithPoolingAndCrossAttentions\n",
    "from transformers import BertConfig\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup, get_constant_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a BERT model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing two sequences A and B\n",
    "sequence_a = \"This is the first sentence for self attention between all tokens of A\"\n",
    "sequence_b = \"This is the second sentence for self attention between all tokens of B\"\n",
    "\n",
    "# # Tokenize the sequences and add special tokens\n",
    "# tokens_a = tokenizer.tokenize(sequence_a)\n",
    "# tokens_b = tokenizer.tokenize(sequence_b)\n",
    "\n",
    "# tokens = '[CLS]' + sequence_a + '[SEP]' + sequence_b + '[SEP]'\n",
    "# text  ='[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(sequence_a, sequence_b, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023,  2003,  1996,  2034,  6251,  2005,  2969,  3086,  2090,\n",
       "          2035, 19204,  2015,  1997,  1037,   102,  2023,  2003,  1996,  2117,\n",
       "          6251,  2005,  2969,  3086,  2090,  2035, 19204,  2015,  1997,  1038,\n",
       "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = bert_model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(outputs.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.9479e-01,  1.2673e-01, -8.1895e-01,  ..., -7.8420e-01,\n",
       "           1.6365e-01,  5.8363e-01],\n",
       "         [-1.0405e+00, -7.1575e-01, -6.0639e-01,  ..., -4.3230e-01,\n",
       "           9.0249e-01,  8.3769e-02],\n",
       "         [-6.3889e-01, -7.4234e-01,  8.2061e-02,  ...,  3.3358e-01,\n",
       "          -5.2194e-02,  9.9931e-01],\n",
       "         ...,\n",
       "         [-1.1168e+00,  2.7644e-01, -5.4177e-01,  ..., -4.9904e-01,\n",
       "          -5.9641e-01,  2.2392e-01],\n",
       "         [ 2.2503e-01,  4.1903e-01, -5.0543e-04,  ..., -7.6442e-01,\n",
       "           2.9230e-01,  2.7904e-01],\n",
       "         [ 9.2042e-01, -9.9951e-03, -5.2279e-01,  ...,  6.1238e-01,\n",
       "          -5.7300e-01, -2.9626e-01]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f3bbdc13f4208d68ca97614a7e728a29123bbfad7733aa1afeab52fbdc3082f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
