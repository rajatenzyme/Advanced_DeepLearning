{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import  BertPooler, BertEmbeddings, BaseModelOutputWithPoolingAndCrossAttentions, BaseModelOutputWithPastAndCrossAttentions,BertPreTrainedModel\n",
    "# from transformers.models.bert.modeling_bert import  BertAttention\n",
    "import torch\n",
    "from torch import nn\n",
    "BertLayerNorm = torch.nn.LayerNorm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bert.modeling_bert import BertEncoder\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from ast import literal_eval\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences    \n",
    "\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW, BertModel, BertConfig\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup, get_constant_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.activations import ACT2FN\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "from transformers.pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = position_embedding_type or getattr(\n",
    "            config, \"position_embedding_type\", \"absolute\"\n",
    "        )\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "\n",
    "        self.is_decoder = config.is_decoder\n",
    "\n",
    "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if is_cross_attention and past_key_value is not None:\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_layer = past_key_value[0]\n",
    "            value_layer = past_key_value[1]\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif is_cross_attention:\n",
    "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif past_key_value is not None:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
    "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        use_cache = past_key_value is not None\n",
    "        # print(\"=============>\")\n",
    "        # print(\"attention_mask is \",attention_mask)\n",
    "        # print(\"==============>\")\n",
    "        \n",
    "        \n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_layer, value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n",
    "            if use_cache:\n",
    "                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n",
    "                    -1, 1\n",
    "                )\n",
    "            else:\n",
    "                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
    "            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
    "            distance = position_ids_l - position_ids_r\n",
    "\n",
    "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "\n",
    "            if self.position_embedding_type == \"relative_key\":\n",
    "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores\n",
    "            elif self.position_embedding_type == \"relative_key_query\":\n",
    "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (past_key_value,)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertCrossAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        # if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "        #     raise ValueError(\n",
    "        #         f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "        #         f\"heads ({config.num_attention_heads})\"\n",
    "        #     )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = position_embedding_type or getattr(\n",
    "            config, \"position_embedding_type\", \"absolute\"\n",
    "        )\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "\n",
    "        self.is_decoder = config.is_decoder\n",
    "\n",
    "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states_A: torch.Tensor,\n",
    "        hidden_states_B: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        mixed_query_layer = self.query(hidden_states_A)\n",
    "\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if is_cross_attention and past_key_value is not None:\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_layer = past_key_value[0]\n",
    "            value_layer = past_key_value[1]\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif is_cross_attention:\n",
    "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif past_key_value is not None:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states_B))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states_B))\n",
    "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
    "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states_B))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states_B))\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        use_cache = past_key_value is not None\n",
    "        # print(\"=============>\")\n",
    "        # print(\"attention_mask is \",attention_mask)\n",
    "        # print(\"==============>\")\n",
    "        \n",
    "        \n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_layer, value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n",
    "            if use_cache:\n",
    "                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states_A.device).view(\n",
    "                    -1, 1\n",
    "                )\n",
    "            else:\n",
    "                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states_A.device).view(-1, 1)\n",
    "            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states_A.device).view(1, -1)\n",
    "            distance = position_ids_l - position_ids_r\n",
    "\n",
    "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "\n",
    "            if self.position_embedding_type == \"relative_key\":\n",
    "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores\n",
    "            elif self.position_embedding_type == \"relative_key_query\":\n",
    "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (past_key_value,)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)\n",
    "        self.cross =BertCrossAttention(config, position_embedding_type=position_embedding_type)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "\n",
    "        # Prune linear layers\n",
    "        self.self.query = prune_linear_layer(self.self.query, index)\n",
    "        self.self.key = prune_linear_layer(self.self.key, index)\n",
    "        self.self.value = prune_linear_layer(self.self.value, index)\n",
    "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "        # Update hyper params and store pruned heads\n",
    "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        \n",
    "\n",
    "        for i in range(16):\n",
    "            attention_mask[0][0][0][i]=1\n",
    "        for i in range(16,31):\n",
    "            attention_mask[0][0][0][i]=0\n",
    "\n",
    "        self_outputs1 = self.self(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "\n",
    "        for i in range(16):\n",
    "            attention_mask[0][0][0][i]=0\n",
    "        for i in range(16,31):\n",
    "            attention_mask[0][0][0][i]=1\n",
    "\n",
    "        self_outputs2 = self.self(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "        new_self_output = self_outputs1+ self_outputs2\n",
    "\n",
    "        # for i in range(31):\n",
    "        #     attention_mask[0][0][0][i]=1\n",
    "\n",
    "        # self_outputs = self.cross(\n",
    "        #     self_outputs1,\n",
    "        #     self_outputs2,\n",
    "        #     attention_mask,\n",
    "        #     head_mask,\n",
    "        #     encoder_hidden_states,\n",
    "        #     encoder_attention_mask,\n",
    "        #     past_key_value,\n",
    "        #     output_attentions,\n",
    "        # )\n",
    "\n",
    "\n",
    "        attention_output = self.output(new_self_output[0], hidden_states)\n",
    "        outputs = (attention_output,) + new_self_output[1:]  # add attentions if we output them\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = BertAttention(config)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.add_cross_attention = config.add_cross_attention\n",
    "        if self.add_cross_attention:\n",
    "            if not self.is_decoder:\n",
    "                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n",
    "            self.crossattention = BertAttention(config, position_embedding_type=\"absolute\")\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        # print(\"=============>\")\n",
    "        # print(\"attention_mask is \",attention_mask)\n",
    "        # print(\"==============>\")\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "\n",
    "        # if decoder, the last output is tuple of self-attn cache\n",
    "        if self.is_decoder:\n",
    "            outputs = self_attention_outputs[1:-1]\n",
    "            present_key_value = self_attention_outputs[-1]\n",
    "        else:\n",
    "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        cross_attn_present_key_value = None\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n",
    "                    \" by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "            cross_attention_outputs = self.crossattention(\n",
    "                attention_output,\n",
    "                attention_mask,\n",
    "                head_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                cross_attn_past_key_value,\n",
    "                output_attentions,\n",
    "            )\n",
    "            attention_output = cross_attention_outputs[0]\n",
    "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
    "\n",
    "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
    "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        # if decoder, return the attn key/values as the last output\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BertEncoder1(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask= None,\n",
    "        head_mask= None,\n",
    "        encoder_hidden_states = None,\n",
    "        encoder_attention_mask = None,\n",
    "        past_key_values = None,\n",
    "        use_cache = None,\n",
    "        output_attentions= False,\n",
    "        output_hidden_states = False,\n",
    "        return_dict = True,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                # if use_cache:\n",
    "                #     logger.warning_once(\n",
    "                #         \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                #     )\n",
    "                #     use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModelV2(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in [Attention is\n",
    "    all you need](https://arxiv.org/abs/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
    "    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
    "    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n",
    "    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n",
    "    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder1(config)\n",
    "\n",
    "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    # @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
    "    # @add_code_sample_docstrings(\n",
    "    #     checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "    #     output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "    #     config_class=_CONFIG_FOR_DOC,\n",
    "    # )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n",
    "            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n",
    "            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n",
    "        use_cache (`bool`, *optional*):\n",
    "            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n",
    "            `past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if self.config.is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        batch_size, seq_length = input_shape\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
    "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
    "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
    "                token_type_ids = buffered_token_type_ids_expanded\n",
    "            else:\n",
    "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
    "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            if encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "        \n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            past_key_values_length=past_key_values_length,\n",
    "        )\n",
    "\n",
    "        \n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_a = \"This is the first sentence for self attention between all tokens of A\"\n",
    "sequence_b = \"This is the second sentence for self attention between all tokens of B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [sequence_a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(sequence_a, sequence_b, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = tokenizer(sequence_b, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2023,  2003,  1996,  2034,  6251,  2005,  2969,  3086,  2090,\n",
       "          2035, 19204,  2015,  1997,  1037,   102,  2023,  2003,  1996,  2117,\n",
       "          6251,  2005,  2969,  3086,  2090,  2035, 19204,  2015,  1997,  1038,\n",
       "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModelV2: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModelV2 from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModelV2 from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModelV2 were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.8.attention.cross.key.bias', 'bert.encoder.layer.9.attention.cross.key.weight', 'bert.encoder.layer.2.attention.cross.query.bias', 'bert.encoder.layer.0.attention.cross.key.bias', 'bert.encoder.layer.8.attention.cross.value.weight', 'bert.encoder.layer.5.attention.cross.value.weight', 'bert.encoder.layer.11.attention.cross.value.bias', 'bert.encoder.layer.9.attention.cross.query.weight', 'bert.encoder.layer.5.attention.cross.key.weight', 'bert.encoder.layer.3.attention.cross.key.weight', 'bert.encoder.layer.1.attention.cross.query.bias', 'bert.encoder.layer.9.attention.cross.value.bias', 'bert.encoder.layer.2.attention.cross.query.weight', 'bert.encoder.layer.6.attention.cross.value.weight', 'bert.encoder.layer.9.attention.cross.query.bias', 'bert.encoder.layer.1.attention.cross.value.weight', 'bert.encoder.layer.5.attention.cross.value.bias', 'bert.encoder.layer.7.attention.cross.key.bias', 'bert.encoder.layer.11.attention.cross.key.bias', 'bert.encoder.layer.10.attention.cross.value.weight', 'bert.encoder.layer.10.attention.cross.query.weight', 'bert.encoder.layer.6.attention.cross.value.bias', 'bert.encoder.layer.4.attention.cross.key.bias', 'bert.encoder.layer.3.attention.cross.key.bias', 'bert.encoder.layer.6.attention.cross.query.weight', 'bert.encoder.layer.4.attention.cross.value.bias', 'bert.encoder.layer.5.attention.cross.query.bias', 'bert.encoder.layer.4.attention.cross.query.bias', 'bert.encoder.layer.0.attention.cross.value.weight', 'bert.encoder.layer.10.attention.cross.value.bias', 'bert.encoder.layer.0.attention.cross.value.bias', 'bert.encoder.layer.3.attention.cross.value.weight', 'bert.encoder.layer.11.attention.cross.value.weight', 'bert.encoder.layer.10.attention.cross.query.bias', 'bert.encoder.layer.9.attention.cross.key.bias', 'bert.encoder.layer.4.attention.cross.query.weight', 'bert.encoder.layer.10.attention.cross.key.bias', 'bert.encoder.layer.0.attention.cross.query.weight', 'bert.encoder.layer.2.attention.cross.value.bias', 'bert.encoder.layer.7.attention.cross.value.bias', 'bert.encoder.layer.5.attention.cross.query.weight', 'bert.encoder.layer.3.attention.cross.query.bias', 'bert.encoder.layer.9.attention.cross.value.weight', 'bert.encoder.layer.2.attention.cross.key.weight', 'bert.encoder.layer.1.attention.cross.query.weight', 'bert.encoder.layer.0.attention.cross.key.weight', 'bert.encoder.layer.7.attention.cross.query.weight', 'bert.encoder.layer.7.attention.cross.value.weight', 'bert.encoder.layer.7.attention.cross.query.bias', 'bert.encoder.layer.2.attention.cross.key.bias', 'bert.encoder.layer.11.attention.cross.query.bias', 'bert.encoder.layer.2.attention.cross.value.weight', 'bert.encoder.layer.4.attention.cross.key.weight', 'bert.encoder.layer.8.attention.cross.key.weight', 'bert.encoder.layer.4.attention.cross.value.weight', 'bert.encoder.layer.1.attention.cross.key.bias', 'bert.encoder.layer.6.attention.cross.key.bias', 'bert.encoder.layer.11.attention.cross.key.weight', 'bert.encoder.layer.6.attention.cross.key.weight', 'bert.encoder.layer.5.attention.cross.key.bias', 'bert.encoder.layer.7.attention.cross.key.weight', 'bert.encoder.layer.1.attention.cross.key.weight', 'bert.encoder.layer.8.attention.cross.value.bias', 'bert.encoder.layer.8.attention.cross.query.weight', 'bert.encoder.layer.0.attention.cross.query.bias', 'bert.encoder.layer.8.attention.cross.query.bias', 'bert.encoder.layer.3.attention.cross.query.weight', 'bert.encoder.layer.10.attention.cross.key.weight', 'bert.encoder.layer.11.attention.cross.query.weight', 'bert.encoder.layer.1.attention.cross.value.bias', 'bert.encoder.layer.6.attention.cross.query.bias', 'bert.encoder.layer.3.attention.cross.value.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "bert_model = BertModelV2.from_pretrained('bert-base-uncased',config=config,add_pooling_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========>\n",
      "attention layer attention mask is torch.Size([1, 1, 1, 31])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is torch.Size([1, 1, 1, 31])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is torch.Size([1, 1, 1, 31])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is torch.Size([1, 1, 1, 31])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is torch.Size([1, 1, 1, 31])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is torch.Size([1, 1, 1, 31])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is torch.Size([1, 1, 1, 31])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is torch.Size([1, 1, 1, 31])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is torch.Size([1, 1, 1, 31])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is torch.Size([1, 1, 1, 31])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is torch.Size([1, 1, 1, 31])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is torch.Size([1, 1, 1, 31])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])\n",
      "==========>\n",
      "==========>\n",
      "attention layer attention mask is tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "==========>\n"
     ]
    }
   ],
   "source": [
    "outputs = bert_model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 31, 768])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6404,  0.1404, -0.8445,  ..., -0.7686,  0.0944,  0.7153],\n",
       "         [-0.9540, -0.7089, -0.5438,  ..., -0.3710,  0.8718,  0.1837],\n",
       "         [-0.5461, -0.7304,  0.1625,  ...,  0.3229, -0.1929,  0.9890],\n",
       "         ...,\n",
       "         [-1.0976,  0.1877, -0.5265,  ..., -0.5127, -0.6237,  0.2542],\n",
       "         [ 0.0029,  0.5032,  0.0606,  ..., -0.8658,  0.1695,  0.2606],\n",
       "         [ 0.8506,  0.0011, -0.5645,  ...,  0.5937, -0.6427, -0.3233]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.6404,  0.1404, -0.8445,  ..., -0.7686,  0.0944,  0.7153],\n",
       "         [-0.9540, -0.7089, -0.5438,  ..., -0.3710,  0.8718,  0.1837],\n",
       "         [-0.5461, -0.7304,  0.1625,  ...,  0.3229, -0.1929,  0.9890],\n",
       "         ...,\n",
       "         [-1.0976,  0.1877, -0.5265,  ..., -0.5127, -0.6237,  0.2542],\n",
       "         [ 0.0029,  0.5032,  0.0606,  ..., -0.8658,  0.1695,  0.2606],\n",
       "         [ 0.8506,  0.0011, -0.5645,  ...,  0.5937, -0.6427, -0.3233]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9908, -0.9170, -0.9999,  0.9919,  0.9940, -0.8737,  0.9970,  0.8595,\n",
       "         -0.9997, -1.0000, -0.9770,  0.9997,  0.9789,  0.9919,  0.9837, -0.9920,\n",
       "         -0.9693, -0.9302,  0.8652, -0.9509,  0.9673,  1.0000, -0.9417,  0.8764,\n",
       "          0.9282,  1.0000, -0.9865,  0.9634,  0.9840,  0.8182, -0.9822,  0.8454,\n",
       "         -0.9908, -0.8269, -0.9999, -0.9989,  0.9380, -0.9044, -0.8204, -0.6433,\n",
       "         -0.9661,  0.8977,  1.0000,  0.8079,  0.9436, -0.8561, -1.0000,  0.8698,\n",
       "         -0.9419,  1.0000,  0.9998,  0.9998,  0.8900,  0.9444,  0.9540, -0.9571,\n",
       "          0.7698,  0.8235, -0.8796, -0.9565, -0.9115,  0.9071, -0.9993, -0.9732,\n",
       "          0.9999,  0.9997, -0.9059, -0.8844, -0.8956,  0.7374,  0.9943,  0.8523,\n",
       "         -0.8842, -0.9249,  0.9992,  0.8884, -0.9528,  1.0000, -0.9691, -0.9836,\n",
       "          0.9994,  0.9994,  0.9567, -0.9963,  0.9972, -1.0000,  0.9790, -0.7163,\n",
       "         -0.9853,  0.8682,  0.9579, -0.8756,  0.9986,  0.9741, -0.9882, -0.9698,\n",
       "         -0.9193, -0.9989, -0.8893, -0.9431,  0.8428, -0.9026, -0.9646, -0.8898,\n",
       "          0.9368, -0.9542, -0.9286,  0.9680,  0.9780,  0.9725,  0.8932, -0.8973,\n",
       "          0.9549, -0.9802,  0.9621, -0.8783, -0.9879, -0.9539, -0.9818,  0.9488,\n",
       "         -0.9350, -0.9321,  0.9820, -0.9853,  0.9163, -0.8962, -1.0000, -1.0000,\n",
       "         -0.9827, -0.9742, -0.9305, -0.8934, -0.9758, -0.9657,  0.9713,  0.9739,\n",
       "          0.8993,  1.0000, -0.9246,  0.9675, -0.9768, -0.9958,  0.9939, -0.9310,\n",
       "          0.9958,  0.9737, -0.9812,  0.8315, -0.9608,  0.9626, -0.9904, -0.8747,\n",
       "         -0.9994, -0.9331, -0.8758,  0.9807, -0.9891, -1.0000, -0.9788, -0.8741,\n",
       "         -0.9503,  0.9666,  0.9919,  0.9287, -0.9535,  0.9331,  0.9934,  0.9401,\n",
       "         -0.9682, -0.9157,  0.9403, -0.8923, -0.9997, -0.9808, -0.9269,  0.8159,\n",
       "          0.9856,  0.9249,  0.8831,  0.9984, -0.8853,  0.9897, -0.9723,  0.9835,\n",
       "         -0.8227,  0.8009, -0.9653,  0.9581, -0.9719,  0.9513,  0.9919, -0.9881,\n",
       "         -0.9204, -0.8907, -0.9071, -0.9386, -0.9970,  0.8917, -0.8683, -0.8332,\n",
       "         -0.8680,  0.9419,  0.9999,  0.9226,  0.9924,  0.9439, -0.9832, -0.9262,\n",
       "          0.8443,  0.8257,  0.8576,  0.9942, -0.9829, -0.8175, -0.9639, -0.9788,\n",
       "          0.7795, -0.9434, -0.8814, -0.9574,  0.9914, -0.9681,  0.9963,  0.9201,\n",
       "         -0.9998, -0.9677,  0.8776, -0.9406,  0.9491, -0.8016,  0.9479,  0.9999,\n",
       "         -0.9668,  0.9914,  0.9781, -0.9998, -0.9300,  0.9916, -0.9089,  0.9823,\n",
       "         -0.9768,  0.9997,  0.9998,  0.9902, -0.9582, -0.9990, -0.9912, -0.9993,\n",
       "         -0.8662,  0.9881,  0.9999,  0.9766,  0.9283, -0.9824, -0.9708,  1.0000,\n",
       "         -0.9128, -0.9604, -0.6315, -0.9499, -0.9894,  0.9992,  0.8351,  0.9810,\n",
       "         -0.9616, -0.9662, -0.9710,  0.9953,  0.8596,  0.9998, -0.9692, -0.9989,\n",
       "         -0.9907, -0.9646,  0.7604, -0.9115, -0.9955,  0.6090, -0.9671,  0.9411,\n",
       "          0.8928,  0.9265, -0.9999,  1.0000,  1.0000,  0.9832,  0.9328,  0.9878,\n",
       "         -1.0000, -0.8020,  1.0000, -1.0000, -1.0000, -0.9745, -0.9839,  0.8412,\n",
       "         -1.0000, -0.8420, -0.7714, -0.9214,  0.9988,  0.9756,  1.0000, -1.0000,\n",
       "          0.8533,  0.9662, -0.9691,  0.9999, -0.9660,  0.9775,  0.9808,  0.9162,\n",
       "         -0.8290,  0.9107, -0.9999, -0.9935, -0.9982, -0.9988,  1.0000,  0.8658,\n",
       "         -0.9801, -0.9569,  0.9370, -0.8711,  0.8979, -0.9719, -0.8870,  0.9953,\n",
       "          0.9878,  0.8570,  0.8754, -0.9313,  0.9055,  0.9879,  0.8792,  0.9521,\n",
       "         -0.9501, -0.9291, -0.9378,  0.8205, -0.9986, -0.9864,  0.9900, -0.9198,\n",
       "          0.9997,  1.0000,  0.9435, -0.9716,  0.9796,  0.9229, -0.9393,  1.0000,\n",
       "          0.9918, -0.9768, -0.9589,  0.9707, -0.9816, -0.9839,  1.0000, -0.8891,\n",
       "         -0.9994, -0.9967,  0.9769, -0.9901,  1.0000, -0.9579, -0.9642,  0.9729,\n",
       "          0.9629, -0.9874, -0.7264,  0.8775, -0.9992,  0.9120, -0.9924,  0.9881,\n",
       "          0.9672, -0.8382,  0.9524, -0.9963, -0.9571,  0.8725, -0.9959, -0.9001,\n",
       "          0.9999,  0.9358, -0.8919,  0.7563, -0.9093, -0.7592, -0.9731,  0.9802,\n",
       "          1.0000, -0.9431,  0.9979, -0.9717, -0.7643,  0.8795,  0.9536,  0.9757,\n",
       "         -0.9011, -0.9477,  0.9976, -0.9996, -0.9847,  0.9633,  0.8747, -0.8808,\n",
       "          1.0000,  0.9698,  0.8557,  0.9351,  1.0000,  0.7778,  0.9280,  1.0000,\n",
       "          0.9873, -0.8585,  0.9667,  0.9871, -1.0000, -0.8832, -0.9401,  0.7118,\n",
       "         -0.9513, -0.8299, -0.9865,  0.9851,  0.9999,  0.9241,  0.9048,  0.9988,\n",
       "          1.0000, -0.6724,  0.9632, -0.9803,  0.9963, -1.0000, -0.9902, -0.8960,\n",
       "         -0.8670, -0.9998, -0.9070,  0.8840, -0.9834,  0.9998,  0.9941, -0.9999,\n",
       "         -0.9896, -0.9784,  0.9961,  0.8276, -1.0000, -0.9847, -0.7845,  0.9929,\n",
       "         -0.9549, -0.9724, -0.9894, -0.9347,  0.9329, -0.8920,  0.9349,  0.9999,\n",
       "         -0.9265, -0.9978, -0.9208, -0.7474, -0.9859,  0.9872, -0.9810, -0.9999,\n",
       "         -0.8601,  1.0000, -0.9176,  0.9999,  0.9393,  0.9802, -0.8584,  0.8145,\n",
       "          0.9999,  0.8924, -0.9997, -0.9999, -0.9909, -0.9463,  0.9619,  0.9962,\n",
       "          0.9993,  0.9337,  0.9841,  0.8405, -0.8503,  0.8810,  1.0000, -0.8457,\n",
       "         -0.8783, -0.9505, -0.8384, -0.8959, -0.9430,  1.0000,  0.8638,  0.9740,\n",
       "         -0.9882, -0.9995, -0.9955,  1.0000,  0.9495, -0.8814,  0.9788,  0.9666,\n",
       "         -0.8515,  0.9953, -0.9151, -0.8432,  0.8852,  0.8392,  0.9588, -0.9642,\n",
       "         -0.9733, -0.9279,  0.9335, -0.9745,  1.0000, -0.9721, -0.8561, -0.8297,\n",
       "         -0.9662,  0.9836,  0.7012, -0.9821, -0.8630,  0.8886,  0.9835,  0.8960,\n",
       "         -0.9677, -0.9811,  0.9998,  0.9988, -0.9999, -0.8937,  0.9605, -0.9884,\n",
       "          0.9689,  1.0000,  0.8933,  0.9806,  0.9186, -0.9457,  0.9309, -0.9570,\n",
       "          0.9811, -0.9737, -0.9436, -0.8531,  0.9103, -0.8992, -0.9366,  0.8258,\n",
       "          0.8155, -0.9502, -0.9609, -0.9171,  0.9300,  0.9932, -0.8587, -0.8574,\n",
       "          0.8052, -0.8455, -0.9912, -0.9069, -0.9715, -1.0000,  0.9621, -1.0000,\n",
       "          0.9918,  0.9878, -0.8531,  0.9387,  0.7814,  0.9910, -0.9264, -0.9999,\n",
       "         -0.9679,  0.9207, -0.9447, -0.9834, -0.9192,  0.9448, -0.8723,  0.8128,\n",
       "         -0.9945,  0.9195, -0.8933,  1.0000,  0.8621, -0.9918, -0.9997,  0.8854,\n",
       "         -0.9358,  1.0000, -0.9984, -0.9614,  0.9222, -0.9932, -0.9602,  0.9163,\n",
       "          0.8366, -0.9814, -0.9999,  0.9872,  0.9987, -0.8909,  0.9385, -0.9149,\n",
       "         -0.9591,  0.7754,  0.9997,  0.9854,  0.9628,  0.9957, -0.8424, -0.9471,\n",
       "          0.9748,  0.9119,  0.9393,  0.8432,  1.0000,  0.8951, -0.9777, -0.8898,\n",
       "         -0.9925, -0.9248, -0.9893,  0.8833,  0.9338,  0.9738, -0.9187,  0.9886,\n",
       "         -0.9997,  0.8090, -0.9882, -0.9981,  0.9096, -0.9665, -0.9858, -0.9767,\n",
       "          0.9805, -0.8936, -0.8175,  0.8526,  0.7715,  0.9584,  0.9180, -1.0000,\n",
       "          0.9710,  0.9477,  0.9999,  0.9561,  0.9881,  0.9470,  0.8832, -0.9851,\n",
       "         -0.9998, -0.9016, -0.8602,  0.9713,  0.9710,  0.9641,  0.9032, -0.8308,\n",
       "         -0.9125, -0.9990, -0.8375, -0.9905,  0.9106, -0.9977, -0.9992,  0.9780,\n",
       "          0.9390, -0.8538, -0.9726, -0.9983,  0.9952,  0.9436,  0.9148,  0.7407,\n",
       "          0.8416,  0.9500,  0.9971,  0.9811, -0.9997,  0.9840, -0.9967,  0.9168,\n",
       "          0.8343, -0.9546,  0.8705,  0.9783, -0.9725,  0.8726, -0.8935, -0.9986,\n",
       "          0.7819, -0.8992,  0.9415, -0.9273, -0.8427, -0.9098, -0.8504, -0.9047,\n",
       "         -0.9855,  0.9488,  0.9625,  0.9582,  0.9935, -0.8558, -0.9887, -0.8283,\n",
       "         -0.9997, -0.9582,  0.9981, -0.8623, -0.9967,  0.9954,  0.8341,  0.8607,\n",
       "          0.9660, -0.9096, -0.9258, -0.9822,  0.9752, -0.8926, -0.9694, -0.9722,\n",
       "          0.9176,  0.8587,  1.0000, -0.9994, -0.9999, -0.8824, -0.9046,  0.8800,\n",
       "         -0.9596, -1.0000,  0.9092, -0.9857,  0.9949, -0.9969,  0.9991, -0.9914,\n",
       "         -0.9986, -0.9026,  0.9521,  0.9991, -0.9353, -0.9808,  0.9479, -0.9298,\n",
       "          1.0000,  0.9453, -0.9855, -0.9663,  0.9684, -0.9994, -0.9404,  0.9812]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "         [-0.6485,  0.6739, -0.0932,  ...,  0.4475,  0.6696,  0.1820],\n",
       "         [-0.6270, -0.0633, -0.3143,  ...,  0.3427,  0.4636,  0.4594],\n",
       "         ...,\n",
       "         [-0.6069,  0.3496,  0.3050,  ...,  0.5009, -0.0898,  0.2719],\n",
       "         [ 0.0622,  0.6019,  0.1948,  ..., -0.5445,  0.0549,  0.3826],\n",
       "         [-0.3961, -0.1537,  0.0993,  ..., -0.1994, -0.1080, -0.2871]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0974,  0.1190, -0.0512,  ...,  0.0773,  0.0974,  0.0662],\n",
       "         [-0.7053,  0.6540,  0.1724,  ..., -0.0864,  0.7801,  0.0446],\n",
       "         [-1.3113, -0.4490, -0.5692,  ...,  0.0294,  0.7217,  0.4653],\n",
       "         ...,\n",
       "         [-0.5051,  0.2916,  0.3401,  ...,  0.2952, -0.1991, -0.0091],\n",
       "         [ 0.0221,  0.5287,  0.0733,  ..., -0.6659, -0.0344,  0.5503],\n",
       "         [-0.0893,  0.1901,  0.1011,  ..., -0.0518,  0.5417, -0.2167]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0547, -0.1043, -0.1613,  ...,  0.1337,  0.1983, -0.0038],\n",
       "         [-1.0854,  0.7336,  0.1838,  ..., -0.1280,  0.7014, -0.0227],\n",
       "         [-1.4413, -0.2368, -0.2440,  ..., -0.0389,  0.6110,  0.3654],\n",
       "         ...,\n",
       "         [-0.7559,  0.4399,  0.3836,  ...,  0.3900, -0.5224, -0.3840],\n",
       "         [-0.2193,  1.0810, -0.0185,  ..., -1.1762,  0.1100,  0.3277],\n",
       "         [-0.1718,  0.2099,  0.1251,  ...,  0.1803,  0.5110, -0.3352]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0530, -0.2325, -0.0571,  ...,  0.1315,  0.1793,  0.0543],\n",
       "         [-0.6223,  0.2228,  0.1433,  ..., -0.3315,  0.4655,  0.1918],\n",
       "         [-1.2694, -0.2947,  0.1719,  ..., -0.1922,  0.4933,  0.4683],\n",
       "         ...,\n",
       "         [-0.9238,  0.3153,  0.5200,  ...,  0.1782, -0.3654, -0.5701],\n",
       "         [-0.5248,  1.1006,  0.3420,  ..., -1.2023, -0.5398, -0.0658],\n",
       "         [-0.0255, -0.0507,  0.1145,  ...,  0.1285,  0.0774, -0.0485]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3196, -0.3805, -0.4052,  ..., -0.0154,  0.2335,  0.4270],\n",
       "         [-0.6132,  0.0576, -0.2327,  ..., -0.0416,  0.2098, -0.2001],\n",
       "         [-1.4680, -0.3521,  0.4190,  ..., -0.2406,  0.4966,  0.6506],\n",
       "         ...,\n",
       "         [-0.8049,  0.6507,  0.1266,  ..., -0.0506, -0.3689, -0.4163],\n",
       "         [-0.5323,  0.7130, -0.1828,  ..., -1.2854, -0.1061, -0.1965],\n",
       "         [-0.0175, -0.0211, -0.0049,  ...,  0.0403,  0.0372, -0.0247]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 2.7484e-01, -1.8810e-02, -5.9316e-01,  ..., -4.5134e-01,\n",
       "           1.0714e-01,  2.5532e-01],\n",
       "         [-7.4850e-01, -4.3455e-02, -5.4440e-01,  ..., -2.6531e-01,\n",
       "           2.2359e-01,  6.0386e-02],\n",
       "         [-1.3757e+00, -3.9805e-01, -4.8362e-02,  ...,  2.4434e-01,\n",
       "           1.0967e-01,  7.5442e-01],\n",
       "         ...,\n",
       "         [-6.1600e-01,  4.1358e-01, -2.5401e-01,  ..., -4.8412e-02,\n",
       "          -4.2447e-01, -1.4614e-01],\n",
       "         [-1.1190e+00,  1.0736e+00,  2.9832e-01,  ..., -1.5941e+00,\n",
       "          -6.5564e-01, -3.6723e-01],\n",
       "         [-2.1115e-03, -9.2022e-03, -1.5664e-03,  ...,  2.4540e-02,\n",
       "          -2.0362e-02, -2.2282e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0977,  0.5939, -1.8256,  ..., -1.0772,  0.4013,  0.2517],\n",
       "         [-1.0318, -0.1240, -1.0028,  ..., -1.0138,  0.2379,  0.2544],\n",
       "         [-1.5095, -0.5453, -0.0492,  ..., -0.0949, -0.1135,  1.4215],\n",
       "         ...,\n",
       "         [-0.7087,  0.5808, -0.0982,  ..., -0.1683, -0.1758, -0.4373],\n",
       "         [-0.7935,  0.7970,  0.3936,  ..., -1.2209, -0.4476, -0.8130],\n",
       "         [ 0.0139,  0.0057, -0.0201,  ...,  0.0284, -0.0287, -0.0306]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0164,  0.0157, -1.2377,  ..., -1.0232, -0.3535,  0.6192],\n",
       "         [-1.5528, -0.2917, -0.8267,  ..., -0.3621, -0.1012,  0.5575],\n",
       "         [-1.4492, -0.5538,  0.2285,  ...,  0.0985, -0.2659,  1.6225],\n",
       "         ...,\n",
       "         [-0.2350,  0.9575, -0.0686,  ..., -0.0342, -0.0723, -0.6161],\n",
       "         [-0.6738,  1.0600,  0.4431,  ..., -1.0944, -0.0380, -0.7887],\n",
       "         [ 0.0352,  0.0210, -0.0183,  ...,  0.0047, -0.0130, -0.0430]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.3250,  0.0090, -1.1227,  ..., -0.7671, -0.1421,  0.6655],\n",
       "         [-1.2039, -0.3314, -0.8452,  ..., -0.0612,  0.0944,  0.6877],\n",
       "         [-1.3692, -0.5016, -0.1319,  ...,  0.1107, -0.2611,  1.4272],\n",
       "         ...,\n",
       "         [-0.5505,  0.5348, -0.1894,  ..., -0.1045, -0.4695, -0.8942],\n",
       "         [-0.3046,  0.8565,  0.4788,  ..., -0.8702, -0.0207, -0.2681],\n",
       "         [ 0.0575,  0.0595,  0.0328,  ..., -0.0327, -0.0586, -0.0128]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.8529, -0.2240, -1.1367,  ..., -0.4120, -0.1298,  0.4204],\n",
       "         [-1.1922, -0.6268, -0.6346,  ...,  0.2237,  0.5470,  0.7760],\n",
       "         [-1.4369, -0.5630, -0.2630,  ...,  0.0651, -0.1546,  1.2987],\n",
       "         ...,\n",
       "         [-0.8737,  0.3090,  0.1423,  ..., -0.1675, -0.6250, -0.9855],\n",
       "         [-0.3949,  0.9992,  0.5737,  ..., -0.9419,  0.1404, -0.2418],\n",
       "         [ 0.0767,  0.1516, -0.0273,  ..., -0.0437, -0.0954,  0.0623]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.0302, -0.2740, -1.0642,  ..., -0.1148, -0.1877,  0.7240],\n",
       "         [-1.5518, -0.4860, -0.5864,  ..., -0.0554,  0.5834,  0.5033],\n",
       "         [-1.4566, -0.5010, -0.4804,  ...,  0.3682, -0.2781,  1.6403],\n",
       "         ...,\n",
       "         [-1.2344,  0.1214,  0.1649,  ...,  0.1493, -0.4311, -0.3613],\n",
       "         [-0.4182,  0.7237,  0.6061,  ..., -1.0585,  0.3392, -0.1763],\n",
       "         [ 0.0178,  0.0201, -0.0751,  ...,  0.0324, -0.0444,  0.0116]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.6482, -0.2730, -0.8454,  ..., -0.3335, -0.0681,  0.9396],\n",
       "         [-1.1236, -0.8469, -0.7437,  ..., -0.7645,  0.7262,  0.5188],\n",
       "         [-1.1140, -0.9140, -0.3549,  ...,  0.0962, -0.1440,  1.7296],\n",
       "         ...,\n",
       "         [-1.1690, -0.0409, -0.1982,  ..., -0.3421, -0.6173,  0.3726],\n",
       "         [ 0.0603,  0.7136,  0.1759,  ..., -1.3645,  0.4522,  0.0789],\n",
       "         [ 0.0576,  0.0303, -0.0563,  ...,  0.0061, -0.0243,  0.0185]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.6404,  0.1404, -0.8445,  ..., -0.7686,  0.0944,  0.7153],\n",
       "         [-0.9540, -0.7089, -0.5438,  ..., -0.3710,  0.8718,  0.1837],\n",
       "         [-0.5461, -0.7304,  0.1625,  ...,  0.3229, -0.1929,  0.9890],\n",
       "         ...,\n",
       "         [-1.0976,  0.1877, -0.5265,  ..., -0.5127, -0.6237,  0.2542],\n",
       "         [ 0.0029,  0.5032,  0.0606,  ..., -0.8658,  0.1695,  0.2606],\n",
       "         [ 0.8506,  0.0011, -0.5645,  ...,  0.5937, -0.6427, -0.3233]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)), past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f3bbdc13f4208d68ca97614a7e728a29123bbfad7733aa1afeab52fbdc3082f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
