{\rtf1\ansi\ansicpg1252\cocoartf2709
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Roman;\f1\fnil\fcharset0 Charter-Bold;\f2\fnil\fcharset0 Charter-Roman;
\f3\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;\red24\green28\blue34;\red255\green255\blue255;\red31\green31\blue31;
\red42\green49\blue64;\red245\green245\blue246;\red24\green28\blue34;}
{\*\expandedcolortbl;;\cssrgb\c12549\c14902\c18039;\cssrgb\c100000\c100000\c100000;\cssrgb\c16078\c16078\c16078;
\cssrgb\c21569\c25490\c31765;\cssrgb\c96863\c96863\c97255;\cssrgb\c12549\c14902\c18039;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs30 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Logistic regression\
PCA\
LDA\
gradients\
regularization\
l1 vs l2 - \
\
L2\
\pard\pardeftab720\partightenfactor0

\f1\b\fs42 \cf4 \cb3 \strokec4 ridge regression shrinks the coefficients and it helps to reduce the model complexity and multi-collinearity.
\f0\b0\fs30 \cf2 \cb3 \strokec2 \
\pard\pardeftab720\partightenfactor0
\cf2 \
\
lasso -  L1 \
\pard\pardeftab720\partightenfactor0

\f2\fs42 \cf4 \cb3 \strokec4 some of the features are completely neglected for the evaluation of output.\'a0
\f1\b \cf4 \cb3 \strokec4 So Lasso regression not only helps in reducing over-fitting but it can help us in feature selection.
\f2\b0 \cf4 \cb3 \strokec4 \'a0
\f0\fs30 \cf2 \cb3 \strokec2 \
\pard\pardeftab720\partightenfactor0
\cf2 \
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0
\f3\fs32 \cf5 \cb6 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec5 Encourages sparsity in the model by forcing some coefficients to become exactly zero.\cb1 \
\ls1\ilvl0\cb6 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec5 Useful for feature selection as it tends to set less important features to zero, effectively reducing the number of features.\cb1 \
\pard\pardeftab720\partightenfactor0

\f0\fs30 \cf2 \cb3 \strokec2 \
\
\
why l2 is better\
\
Normalization - Layer Batch\
Gradient Descents - Batch Mini Batch SGD Adagrad RMSProp\
NN\
\
Difference between boosting and bagging.\
\
Formula for EMD distance\
Formula for KL-divergence\
Explain VAE - step-by-step\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
Gfg - ML\
conversational AI\
\
\
\
\
\
\
\
\
\
nn.Embedding(num_embeddings, embed_dim)\
\
\
\
5 feats - 1 cat , 4 num\
\
\
row - [1000, 14], 4 num\
\
5th category - \
\
14, 4 num\
\
14 + 4  = 18\
\
semantic meaning\
\
Q2) title - > bunch of words - > \
\
tokenizer -> \
\
 - > 1024\
\
padding - \
\
5 words - > [1,2,,3,4,5,1,3,3,4,4] \
10 words -> \
\
words - \
\
\'93excellent shoes\'94 -> \
\
w2v - \
\
\'93I am a good boy\'94  [] [] [] \cf7 \cb3 \outl0\strokewidth0 [] [] -> \cf2 \cb3 \outl0\strokewidth0 \strokec2 \
\'93I maybe a good boy\'94 \cf7 \cb3 \outl0\strokewidth0 [] [] [] [] [] ->  \cf2 \cb3 \outl0\strokewidth0 \strokec2 \
\
\
data - \
\
model - \
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
\
}